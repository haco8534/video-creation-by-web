# 【台本】AIは絵を"描いて"ない？拡散モデルによる画像生成の仕組み

- 作成日: 2026-02-25
- 目標尺: 約20分
- コアメッセージ: 拡散モデルは「ノイズを加えて壊す→元に戻す」という原理でノイズの中からパターンを発見しており、AIが絵を描いているのではなくノイズから画像を見つけ出しているという理解が正しい
- 冒頭フック: 「AIが描いた絵、実は砂嵐のテレビから始まってるって知ってました？」
- 登場人物: ずんだもん（解説）、めたん（聞き役）
- セリフ総文字数: 約6,400文字（目標：6,000〜7,000文字）
- シーン遷移マーカー: あり（`<!-- SCENE: ... -->` 形式で各ブロック内に配置済み）

---

# ▶ 導入

## 【Block 1：オープニング「AIは"描いて"ない？」】
<!-- 推定時間：2分 / 目標文字数：600〜700文字 -->

<!-- SCENE: タイトルカード「拡散モデルによる画像生成の仕組み」 -->

めたん：今日はどんなお話ですの？
ずんだもん：今日は、画像生成AIがどうやって絵を作っているのか、その仕組みを全部解説するのだ。
めたん：Stable DiffusionとかMidjourneyとか、最近すごいですわよね。テキストを入れるだけで絵が出てくるんですもの。
ずんだもん：そうなのだ。でもここで一つ質問なのだ。AIが描いた絵って、実は砂嵐のテレビから始まってるって知ってたのだ？

<!-- SCENE: テキスト強調「AIは"描く"のではなく"見つけ出す"」 -->

めたん：え？砂嵐？あの地上波が終わった後に出てくるザーッていうやつですの？
ずんだもん：そうなのだ。多くの人は「AIが人間みたいに筆を動かして絵を描いている」とか、「ネット上の画像を切り貼りしている」と思っているのだ。でも実際は全然違うのだ。
めたん：じゃあ何をしてるんですの？
ずんだもん：AIは、完全にランダムなノイズ、つまり砂嵐みたいな画像の中から、少しずつパターンを見つけ出して、画像を浮かび上がらせているのだ。描いているんじゃなくて、「見つけ出している」のだ。
めたん：砂嵐から画像を見つけ出す……にわかには信じがたいですわね。
ずんだもん：不思議に聞こえるだろう？でもこれは比喩じゃなくて本当にそうなのだ。今日はその仕組みを「拡散モデル」の原理から全部説明するのだ。この動画を見終わる頃には、画像生成AIの中で何が起きているのか、全部わかるようになるのだ。
めたん：楽しみですわ！早く教えてくださいな。
ずんだもん：もちろんなのだ。でもその前に、ちょっとだけ歴史の話をさせてほしいのだ。そもそも「画像を自動で作るAI」ってのは拡散モデルが最初じゃないのだ。

## 【Block 2：前提知識「画像生成AIの三世代」】
<!-- 推定時間：2〜3分 / 目標文字数：700〜900文字 -->

<!-- SCENE: フロー図「画像生成AIの歴史 VAE(2013)→GAN(2014)→拡散モデル(2020)」 -->

ずんだもん：画像を自動で作り出すAIの研究は、ざっくり3つの世代に分けられるのだ。最初は2013年のVAE、次が2014年のGAN、そして今の主流が2020年に実用化された拡散モデルなのだ。
めたん：GANって聞いたことありますわ。一時期話題になってましたわよね。
ずんだもん：そうなのだ。GANは約6年間、画像生成AIの王様だったのだ。

<!-- SCENE: 比較対照「GAN（偽造者vs鑑定士）vs 拡散モデル（お掃除ロボット）」 -->

ずんだもん：GANの仕組みを簡単にたとえると、「偽造者と鑑定士の対決」なのだ。偽造者が偽物の画像を作って、鑑定士が「これは偽物だぞ」と見破ろうとする。この対決を何十万回も繰り返すことで、偽造者の腕がどんどん上がって、最終的には本物そっくりの画像が作れるようになるのだ。
めたん：面白い仕組みですわね。でもそれが主流じゃなくなったのはなぜですの？
ずんだもん：対決形式には大きな弱点があったのだ。偽造者と鑑定士のバランスを取るのがものすごく難しくて、片方が強くなりすぎるとすぐに学習が破綻するのだ。
めたん：どちらかが圧勝してしまうと成り立たないんですのね。
ずんだもん：そうなのだ。さらに「モード崩壊」って言って、偽造者が「この手の画像なら鑑定士を騙せる」って学習してしまうと、同じような画像ばかり生成してしまう問題もあったのだ。

<!-- SCENE: テキスト強調「拡散モデルが主流になった理由→学習の安定性」 -->

めたん：なるほど……それは困りますわね。で、拡散モデルはどう違うんですの？
ずんだもん：拡散モデルは対決なんか一切しないのだ。やっていることはもっとシンプルで、「画像に加えられたノイズを予測する」という1つのタスクだけを学習するのだ。対戦相手がいないから、バランス調整の問題が起きない。だから学習が非常に安定していて、多様な画像を生成できるのだ。
めたん：対決じゃなくて、お掃除みたいな感じですのね。汚れを見つけて取り除くだけ。
ずんだもん：まさにそうなのだ。イメージとしてはお掃除ロボットに近いのだ。で、この拡散モデルが具体的にどんな仕組みで動いているのか、ここからが本題なのだ。

---

# ▶ 本論

## 【Block 3：トピック①「拡散モデルの基本原理」】
<!-- 推定時間：4〜5分 / 目標文字数：1200〜1400文字 -->

<!-- SCENE: タイトルカード「拡散モデルの基本原理」 -->

めたん：いよいよ本題ですわね。拡散モデルって、そもそもどういう仕組みなんですの？
ずんだもん：拡散モデルの原理は、実は物理学から来ているのだ。2015年にスタンフォード大学のソール・ディクスタインっていう研究者が、非平衡熱力学っていう物理の分野からヒントを得て考案したのだ。
めたん：非平衡熱力学……難しそうですわね。
ずんだもん：言葉は難しいけど、原理はシンプルなのだ。水にインクを一滴垂らすと、最初はくっきりした色の塊なのに、時間が経つとだんだん広がって、最終的には全体が均一にぼんやりしてしまうのだ。これが物理学でいう「拡散」で、拡散モデルの名前の由来なのだ。
めたん：なるほど。インクが広がるイメージですのね。それが画像生成と何の関係がありますの？

<!-- SCENE: フロー図「きれいな画像→徐々にノイズ→完全なノイズ（Forward Process）」 -->

ずんだもん：拡散モデルでは、きれいな画像にちょっとずつノイズを加えていくのだ。ステップ1では画像がほんの少しだけザラザラする。ステップ2ではもう少しザラザラになる。これを何百回も繰り返すと、最終的には元の画像が何だったか全くわからない、完全なノイズ、つまり砂嵐になるのだ。
めたん：わざと画像を壊していくんですの？何のためにそんなことを？
ずんだもん：いい質問なのだ。この「壊す過程」はフォワードプロセスって呼ばれていて、ここではAIの学習は何も行われないのだ。ただ機械的にノイズを乗せていくだけなのだ。大事なのはこの次なのだ。

<!-- SCENE: フロー図「完全なノイズ→徐々にくっきり→画像が復元される（Reverse Process）」 -->

ずんだもん：今度は逆に、完全なノイズの状態からスタートして、少しずつノイズを取り除いていくのだ。ステップ1ではうっすらと輪郭が見え始める。ステップ2ではもう少し形がはっきりしてくる。これを繰り返していくと、最終的にはきれいな画像が浮かび上がってくるのだ。
めたん：壊す→戻すってことですのね。まるで彫刻みたいですわ。大理石の塊から像を掘り出すような。
ずんだもん：おっ、いいたとえなのだ。実際そのイメージに近いのだ。で、ここで重要なのは「AIはどうやって戻し方を知るのか」なのだ。
めたん：そうですわね。壊すのは簡単でも、戻すのは難しそうですわ。
ずんだもん：ここがポイントなのだ。AIに大量の画像で「壊す→戻す」の練習をさせるのだ。何百万枚もの画像に対して、いろんな段階でノイズを加えて、そのノイズを取り除く練習を繰り返すのだ。猫の画像でも風景の画像でも、ありとあらゆる画像で練習するのだ。

<!-- SCENE: テキスト強調「学習の目標はたった1つ：ノイズεを予測すること」 -->

めたん：でもそれって膨大な計算になりませんの？
ずんだもん：実はここに美しい発見があるのだ。2020年にジョナサン・ホーらの研究チームが発表したDDPMっていう論文で、学習の目標をものすごくシンプルにできることが示されたのだ。やるべきことはたった1つ、「この画像にどんなノイズが加えられたか」を予測するだけなのだ。
めたん：ノイズを予測するだけ！？それだけであんなすごい画像が作れるんですの？
ずんだもん：そうなのだ。数学的にはイプシロンって呼ばれるノイズ成分を予測するだけで、結果的に画像全体を復元できることが証明されたのだ。画像を丸ごと予測するんじゃなくて、「このステップでどれだけ汚れが乗ったか」だけ当てればいいのだ。
めたん：シンプルだけど奥が深いですわね……
ずんだもん：このシンプルさこそが、拡散モデルの安定した学習を可能にしている鍵なのだ。GANみたいに敵同士を戦わせる必要がないから、学習が途中で壊れにくいのだ。
めたん：なるほど……原理はよくわかりましたわ。でも実際のStable Diffusionとかって、もうちょっと複雑なんじゃありませんの？
ずんだもん：いいところに気づいたのだ。実は、今の原理をそのまま使うと計算コストが大きすぎるのだ。512かける512の画像で直接ノイズ除去をすると、とてもじゃないけど一般のPCでは動かないのだ。だから実際の製品にはいくつもの天才的な工夫が加えられているのだ。

## 【Block 4：トピック②「Stable Diffusionの内部構造」】
<!-- 推定時間：4〜5分 / 目標文字数：1200〜1400文字 -->

<!-- SCENE: フロー図「Stable Diffusionの全体像：VAE圧縮→U-Net→VAE復元」 -->

ずんだもん：Stable Diffusionの中身には、大きく分けて3つの部品があるのだ。まずVAEっていう圧縮装置、次にU-Netっていうノイズ除去エンジン、そしてまたVAEの復元装置なのだ。
めたん：3つの部品が連携してるんですのね。まず最初のVAEって何をするんですの？

<!-- SCENE: 数値インパクト「512x512の画像を64x64に圧縮（約64分の1）」 -->

ずんだもん：VAEは画像を圧縮する役割なのだ。例えば512かける512ピクセルの画像があったとするだろう。これは約78万ピクセルもあるのだ。直接ノイズ除去しようとすると、とんでもない計算量が必要になるのだ。
めたん：78万ピクセル……確かに多いですわね。
ずんだもん：だからVAEが画像を「潜在空間」っていう小さな世界に圧縮するのだ。具体的には64かける64まで縮小するのだ。データ量にして約64分の1なのだ。
めたん：64分の1ですの！？そんなに小さくして大丈夫なんですの？
ずんだもん：VAEは賢いのだ。ただ画像を縮小するんじゃなくて、画像の「本質的な特徴」、つまり形や色の傾向、テクスチャの情報だけを保存するのだ。いわば画像の「エッセンス」だけ取り出すイメージなのだ。細かいピクセル情報は捨てるけど、意味のある情報はちゃんと残すのだ。

<!-- SCENE: フロー図「テキスト"猫"→CLIPエンコーダ→ベクトル→Cross-Attention→U-Netに指示」 -->

ずんだもん：次がU-Netなのだ。これがノイズ除去の本体で、名前の通りU字の形をしたニューラルネットワークなのだ。圧縮された画像のノイズを予測して取り除くのだ。ここで面白いのは、U-Netにはテキストの情報も入ってくるのだ。
めたん：テキストの情報？プロンプトのことですわね。どうやって文字の情報を画像生成に使っているんですの？
ずんだもん：ユーザーが入力したプロンプト、例えば「夕焼けの海辺を歩く猫」は、まずCLIPっていう別のAIがテキストを数値のベクトルに変換するのだ。
めたん：言葉を数字に変えるんですのね。
ずんだもん：そうなのだ。そしてそのベクトルが、U-Netの中にあるクロスアテンションっていう仕組みを通じて、「こういう雰囲気の画像にしてくれ」という指示を出すのだ。ノイズを除去するときに、テキストの意味を参考にしながら方向性を決める、いわばガイド役なのだ。
めたん：テキストがAIへのナビゲーターになるんですのね。砂嵐の中から「猫っぽいもの」を探し出すためのヒントを与えてくれる。
ずんだもん：まさにそうなのだ。さらにClassifier-Free Guidanceっていうテクニックで、プロンプトにどれくらい忠実に従うかも調整できるのだ。

<!-- SCENE: 数値インパクト「DDIMで1000ステップ→50ステップに高速化（20倍速）」 -->

ずんだもん：次に速度の工夫なのだ。元々の拡散モデルは、ノイズを除去するのに1000ステップも必要だったのだ。1枚の画像を作るのに1000回もAIを動かさないといけないのだ。
めたん：それはさすがに遅すぎますわね……
ずんだもん：だからDDIMっていう高速化手法が開発されたのだ。これは1ステップずつ地道に進む代わりに、大きくジャンプしながらノイズを除去する方法なのだ。これを使うと、1000ステップを約50ステップまで減らせるのだ。約20倍の高速化で、しかも画質はほとんど落ちないのだ。
めたん：1000が50に！それはすごい進歩ですわね。
ずんだもん：こうした工夫、VAEによる圧縮、U-Netとクロスアテンション、そしてDDIMによる高速化が全部組み合わさったおかげで、Stable Diffusionは一般の人のパソコンでも動くようになったのだ。
めたん：すごい技術の積み重ねですわね。
ずんだもん：でもね、そんな拡散モデルにも、どうしても苦手なことがあるのだ。

## 【Block 5：トピック③「AIの限界と誤解を解く」】
<!-- 推定時間：3〜4分 / 目標文字数：1000〜1200文字 -->

<!-- SCENE: タイトルカード「AIの限界と誤解」 -->

めたん：苦手なこと？そういえばAIが描いた絵って、手がおかしいことが多いですわよね。指が6本あったり、変な方向に曲がってたり。
ずんだもん：まさにそれなのだ。実はAIが手を描くのが苦手なのには、ちゃんとした理由があるのだ。

<!-- SCENE: 段階的リスト「手が描けない3つの理由」 -->

ずんだもん：理由は大きく3つあるのだ。1つ目は、手の構造がめちゃくちゃ複雑だってこと。骨だけでも27本あって、関節は指だけで14個。指の曲がり方の組み合わせが膨大すぎるのだ。
めたん：確かに、手って体の中でも特に複雑な部位ですわよね。
ずんだもん：2つ目は、学習データの問題なのだ。ネット上の画像って、手が小さく写っていたり、ぼやけていたり、物を持って隠れていたりすることが多いのだ。だから十分な品質で手のデータを学習できていないのだ。
めたん：確かに、写真って顔はアップで写っても手はそうでもないですわね。
ずんだもん：そして3つ目、これが一番根本的な理由なのだ。AIは「指は5本」っていう知識を持っていないのだ。

<!-- SCENE: 注意喚起「よくある誤解：AIは画像をコラージュしている？→実際は確率分布の学習」 -->

めたん：え？知らないんですの？人間なら当たり前のことですのに。
ずんだもん：そうなのだ。拡散モデルは画像のパターンの統計的な傾向を学んでいるだけで、「手には指が5本ある」とか「関節はこの方向にしか曲がらない」という構造的な知識は一切持っていないのだ。だから「このあたりは指っぽいパターンだな」程度の理解で描いてしまうから、時々変な形が出てくるのだ。
めたん：パターンは知ってるけどルールは知らない、ということですのね。そういえば、「AIはネット上の画像を切り貼りしている」っていう話も聞きますけど、それは本当ですの？
ずんだもん：それは完全な誤解なのだ。拡散モデルは個別の画像を記憶しているわけじゃないのだ。何百万枚もの画像から「画像っぽさの確率分布」を学んでいるのだ。たとえて言うなら、何千冊もの料理本を読んだ人が、レシピを暗記しているんじゃなくて、「料理のパターン」を身につけてオリジナル料理を作るようなものなのだ。
めたん：なるほど。コラージュでもコピーでもないんですのね。
ずんだもん：ただし、学習データに使われた画像の著作権をめぐっては大きな議論が起きているのだ。写真素材の大手Getty ImagesがStability AIを著作権侵害で訴えた事例もあるし、多くのイラストレーターが自分の作品が無断で学習に使われたと抗議しているのだ。

<!-- SCENE: 引用カード「学習データと著作権をめぐる議論」 -->

めたん：技術的にはコピーじゃなくても、学習に使ったこと自体が問題になるケースがあるんですのね。
ずんだもん：そうなのだ。「分布を学んでいるだけだから問題ない」という主張と、「許可なく学習すること自体が権利侵害だ」という主張がぶつかっていて、今まさに世界中で法的な判断が進んでいるところなのだ。技術の仕組みを正しく理解した上で、こうした問題にも目を向けていくことが大事なのだ。

---

# ▶ 終わりに

## 【Block 6：まとめ・コアメッセージ】
<!-- 推定時間：2〜3分 / 目標文字数：600〜800文字 -->

<!-- SCENE: まとめカード「拡散モデルの3つのポイント」 -->

ずんだもん：さて、今日の話をまとめるのだ。拡散モデルのポイントは3つなのだ。1つ目、拡散モデルは「きれいな画像にノイズを加えて壊す」「逆にノイズを取り除いて復元する」の2段階で動いていること。
めたん：Forward ProcessとReverse Processですわね。
ずんだもん：2つ目、Stable Diffusionでは画像を64分の1に圧縮してからノイズ除去を行い、テキストはCLIPでベクトルに変換してU-Netに指示を出していること。
めたん：だから一般のPCでも動くんですわね。
ずんだもん：3つ目、AIは画像を「描いている」のではなく、ノイズの中から統計的なパターンを「見つけ出している」ということ。だから手の描画が苦手だったり、構造的な理解はしていなかったりするのだ。

<!-- SCENE: 横並びカード「動画生成Sora・3D生成DreamFusion・音声合成」 -->

めたん：動画の最初に言っていた「砂嵐から始まる」っていうのは、こういう意味だったんですのね。
ずんだもん：そうなのだ。AIが生成する画像は、文字通り砂嵐から始まっているのだ。そしてこの拡散モデルの技術は、今や画像だけにとどまらないのだ。OpenAIのSoraはテキストから動画を生成するし、DreamFusionはテキストから3Dモデルを作るし、音声合成にも拡散モデルが使われ始めているのだ。
めたん：画像だけじゃなく、動画も3Dも音声も全部同じ原理から来てるんですのね。すごいですわ。
ずんだもん：しかもStable Diffusionはオープンソースで公開されていて、誰でも無料で使えるのだ。これによって画像生成AIが一気に世界中に広まったのだ。
めたん：技術の民主化ってやつですわね。
ずんだもん：今日の動画で、画像生成AIの中身がブラックボックスじゃなくなったと思うのだ。「なんかすごい」じゃなくて「こうやって動いてたのか」って思えたなら、この動画は成功なのだ。
めたん：今日もとっても勉強になりましたわ。ありがとうございました！
ずんだもん：こちらこそなのだ。次回もお楽しみになのだ！
