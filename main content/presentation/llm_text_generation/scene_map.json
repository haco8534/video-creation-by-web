{
    "voicevox_url": "http://localhost:50021",
    "speakers": {
        "ずんだもん": 3,
        "めたん": 2
    },
    "speed_scale": 1.15,
    "inter_line_silence": 0.3,
    "scene_end_padding": 0.5,
    "scenes": [
        {
            "id": 0,
            "title": "タイトル",
            "hold_sec": 4,
            "lines": []
        },
        {
            "id": 1,
            "title": "衝撃の事実",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "ねえ、ずんだもん。最近仕事でもプライベートでもChatGPTをよく使うんだけど、あれって本当に何でも知ってて賢いよね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "ふふん、AIが人間みたいに言葉を理解しているって、みんなそう思っているのだ。でもめたん、実はここで衝撃的な事実を教えるのだ。"
                }
            ]
        },
        {
            "id": 2,
            "title": "確率的オウム",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "ChatGPTをはじめとする大規模言語モデル、いわゆるLLMは、僕たちが入力した文章の意味を、実は1ミリも理解していないのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "ええっ！？嘘でしょ！？ だって今日の東京の天気はどう？って聞いたら、ちゃんと晴れです。傘はいりませんよ、なんて気の利いたことまで答えてくれるじゃない。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "そこが現代のAIの最も面白くて、恐ろしいところなのだ！ 実はAIの界隈では、LLMのことを確率的オウムなんて呼んだりする批判もあるくらいなのだ。"
                }
            ]
        },
        {
            "id": 3,
            "title": "今日の旅路",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "オウム返し？ でも、ただのオウム返しには見えないわよ。じゃあ一体、AIの中で何が起きているの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "それを今から解き明かすのだ！ 今日は、めたんがパソコンやスマホで東京の天気は？と文字を入力してから、AIが回答の文字を出力するまでの全ステップを、最初から最後まで一直線に追っていくのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "おおー！ 入力した言葉がどうやって処理されて、どうやって答えになって返ってくるのか。データの流れが目で見えるようになるってことね！"
                }
            ]
        },
        {
            "id": 4,
            "title": "出発",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "これを見終わる頃には、AIがまったく意味を理解していないのに、どうしてあんなに賢く見えるのか、その本当のからくりが完全に腑に落ちるはずなのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "楽しみ！ それじゃあ、さっそく教えてちょうだい！"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "それでは、AIの脳内に飛び込んでみるのだ。でもその前に、文字はそのままではコンピュータには読めないのだ。"
                }
            ]
        },
        {
            "id": 5,
            "title": "入力",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "それじゃあ、まずは第一のステップ、入力から始まるのだ。めたんが東京の天気は？と入力して送信ボタンを押した直後を想像してほしいのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "うん。文字がインターネットを通って、AIのサーバーに届くのよね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "そうなのだ。でも、AIの脳みそとなっているコンピュータは、僕たちが使う日本語の文字をそのまま読めるわけじゃないのだ。コンピュータが理解できるのは数字だけなのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "あ、それは聞いたことあるわ。0と1の世界ってやつよね。"
                }
            ]
        },
        {
            "id": 6,
            "title": "Tokenization",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "なのだ。だから、まずは文章を数字に変えるための準備作業が必要なのだ。これをトークン化と呼ぶのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "トークン化？ なんだか仮想通貨みたいな名前ね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "英語でしるしとか断片って意味なのだ。文章を、AIが扱いやすい小さなレゴブロックに分解する作業だと思えばいいのだ。例えば東京の天気は？という文章なら、東京、の、天気、は、？みたいに切り分けるのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "なるほど。単語ごとにバラバラにするのね。"
                }
            ]
        },
        {
            "id": 7,
            "title": "Token ID",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "よく使われる文字の並びごとに区切るBPE、バイトペアエンコーディングっていう賢い仕組みが使われているのだ。こうして切り分けた1つ1つのブロックをトークンと呼ぶのだ。そして、このトークンに、それぞれ背番号みたいなID番号を割り振るのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ふむふむ。東京は105番、のは12番、みたいに辞書の番号に置き換えるのね。これならコンピュータも読めそうね。"
                }
            ]
        },
        {
            "id": 8,
            "title": "Word Embedding",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "でも、まだこれだけじゃ甘いのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "えっ、番号にするだけじゃダメなの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "ただの連番の背番号だと、1番の言葉と2番の言葉に意味の繋がりがあるとは限らないのだ。AIに言葉の関係性を教えるための魔法、第二のステップ、埋め込み、英語でワードエンベディングを行うのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "埋め込み？ どこに何を埋め込むのよ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "さっきのトークンを、膨大な次元を持つ空間の中に埋め込むのだ。空間の座標を考えてほしいのだ。エックス、ワイ、ゼットみたいな。これを数千次元という長い数字のリストに変換するのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "数千次元！？ 三次元までしか想像できないわよ！"
                }
            ]
        },
        {
            "id": 9,
            "title": "意味空間",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "僕たちには想像できない空間だけど、コンピュータには計算できるのだ。この空間のすごいところは、意味が似ている言葉は、空間の中でも近い座標に配置されるってことなのだ。犬と猫は近い場所、犬と消しゴムは遠い場所、という具合に表すのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "へええ！ つまり、ただの連番の背番号から、言葉の意味の近さを表す長い数字のリストにパワーアップさせるってことね！"
                }
            ]
        },
        {
            "id": 10,
            "title": "心臓部へ",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "その通りなのだ！ さらに文章の並び順の情報も付加されるのだ。こうして、僕たちの文章は、意味と順番を持った巨大な数字の集まりに変身したのだ。そして住所が決まった数字のブロックたちは、次にいよいよ、かつてない革命をもたらしたAIの本当の心臓部へと送り込まれるのだ！"
                }
            ]
        },
        {
            "id": 11,
            "title": "Transformer",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "いよいよ心臓部ね！ ゴクリ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "ここからが第三のステップ。現代の生成AIを劇的に賢くした大発明、トランスフォーマーというネットワークの内部に入っていくのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "トランスフォーマー！ 映画みたいな名前ね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "2017年にGoogleの研究者たちが発表したAttention Is All You Needっていう伝説の論文で提案された仕組みなのだ。今のChatGPTもGeminiも、ぜーんぶこの1本の論文から生まれたと言っていいのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "ええっ！ 2017年のたった1つの発明が、今のAIブームのまさに原点になっているの！？ すごすぎるわ！"
                }
            ]
        },
        {
            "id": 12,
            "title": "RNNの限界",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "かつてのAI技術、RNNなどは、文章を最初から順番に1文字ずつ処理していたのだ。だから長文を読むと、最後の方には最初の方のことを忘れちゃう弱点があったし、処理スピードも遅かったのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "鳥頭すぎない？ それじゃあ長い会話なんて無理ね。"
                }
            ]
        },
        {
            "id": 13,
            "title": "Self-Attention",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "そこでTransformerは、Self-Attention、自己注意機構という必殺技を使ったのだ。これを使うと、文章の中のすべての単語を、いっぺんに同時に見渡すことができるようになったのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "全部同時に！？ それってどういうこと？"
                }
            ]
        },
        {
            "id": 14,
            "title": "文脈を読む",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "文字通り、すべての単語がそれぞれ、他のすべての単語とどれくらい関連しているかを計算するのだ。たとえば、銀行でお金を下ろすの銀行と、川のバンクで休むのバンク。"
                },
                {
                    "speaker": "めたん",
                    "text": "ああ、金融機関のバンクと、土手のバンクね。単語だけ見たらどっちかわからないわね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "でもSelf-Attentionは、周りの単語を同時に見渡すのだ。お金や下ろすという単語があれば、あ今回はそっちと強く関係しているなとスコア計算して、これは金融機関のバンクだ！と文脈を読み取ることができるのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "文脈を数字のスコアで理解してるってことね！ でも、どうやって計算してるの？"
                }
            ]
        },
        {
            "id": 15,
            "title": "Q/K/V",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "ここでさっきの埋め込みで作った数字のベクトルが活躍するのだ。各トークンは、クエリ、キー、バリューという3つのベクトルを新しく作り出すのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "クエリ、キー、バリュー。横文字が多くなってきたわ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "合コンで例えるのだ！ クエリは僕はこういう人が好きですという自分の希望。キーは私はこういう人間ですというアピールポイントの名札。そして全員のクエリとキーを掛け合わせて、相性スコアを計算するのだ。相性が良ければ、その人のバリュー、中身の情報を自分の中に強くブレンドするのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "なるほど！ 単語同士で合コンして、お前の情報を私に混ぜてやるってやってるのね。例えが独特だけどわかりやすいわ！"
                }
            ]
        },
        {
            "id": 16,
            "title": "Multi-Head",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "さらにマルチヘッド・アテンションと言って、この合コンを異なる趣味のグループで同時に何十個も並行して行うのだ。文法の関係を見るグループ、感情の関係を見るグループみたいに、いろんな視点から文脈を分析するのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "徹底的に単語同士の関係性を洗い出してるのね。"
                }
            ]
        },
        {
            "id": 17,
            "title": "層の積み重ね",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "このAttentionが終わったら、さらにデータを微調整するレイヤー正規化や、データが消えないようにする残差接続、そして複雑なパターン処理をするフィードフォワード・ネットワークという層を通すのだ。要するに、Attentionで集めた周りの情報を整理整頓して、さらに深い意味合いとして練り上げるという工程を、何十層にも渡って繰り返しているのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ミルフィーユみたいに何層も何層も、情報の関係性をこねくり回してるってことね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "こうして東京の天気は？という入力の裏にある深い文脈を、巨大な数字の塊として見事に構築し終えたのだ。しかし！文脈をこねくり回したAIが次にやることは、実はものすごく単純な作業なのだ。しかもその選び方に、AIが時々とんでもない嘘をつく原因が隠されているのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "えっ！あんなに複雑な計算をしたのに、次は単純なの？ しかも嘘をつく原因があるって……どういうこと？ 次も早く教えて！"
                }
            ]
        },
        {
            "id": 18,
            "title": "次の1トークン予測",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "さあ、文脈を極限まで読み取った巨大な数字の塊が、どうやって僕たちの読める答えの文章になるのか。第四のステップなのだ。めたんは、AIが回答の文章を一気に作っていると思っていなかったのだ？"
                },
                {
                    "speaker": "めたん",
                    "text": "え？ 思ってたわよ。だってチャットの画面にパッと出てくるじゃない。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "実は違うのだ。LLMは、たったひとつのことしかしていない。今まで入力された文章の続きとして、次に来る確率が一番高い1トークンを予測すること、これだけなのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "ええっ！？ つまり1文字ずつ作ってるの！？"
                }
            ]
        },
        {
            "id": 19,
            "title": "確率分布",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "そうなのだ。東京の、天気は、？と入力されたら、その文脈の情報から計算して、LLMが持っている何万語という辞書の中から次に来る言葉の確率一覧表、確率分布を作るのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "確率一覧表？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "たとえば、晴が40%、曇が30%、雨が20%、本が0.01%みたいに、次に続きそうなトークンすべての確率を計算するのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ああ、スマホの予測変換みたいなものね！ それの最強に頭がいいバージョンってことね。"
                }
            ]
        },
        {
            "id": 20,
            "title": "Autoregressive",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "まさにそうなのだ！ この確率一覧表からサンプリング、抽出処理をして、次の言葉をひとつ選ぶのだ。たとえば一番確率の高い晴を選んだとするのだ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "そしたら次は、入力情報が東京の天気は？晴になるのだ。その文脈を再びAttentionのネットワークにぶち込んで、また確率計算をする。これを延々とループで繰り返しているのだ！ これを専門用語で自己回帰、オートリグレッシブ型と呼ぶのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ちょっと待って！ じゃあChatGPTがダーッと長い文章を書いてる時って、はちゃめちゃなスピードで予測ループをブン回してるだけなの！？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "おっしゃる通りなのだ！ そうやって1ステップずつ出力しているのだ。"
                }
            ]
        },
        {
            "id": 21,
            "title": "Temperature",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "信じられない！ でも、一番確率が高い文字を選び続けるだけなら、同じ質問には毎回まったく同じ答えにならない？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "いい質問なのだ。実はここにはサイコロの目の重みづけみたいな仕掛けがあるのだ。Temperatureや、Top-pと呼ばれるパラメータなのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "温度？ AIが熱くなったりするの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "ここでいう温度は予測のランダムさなのだ。温度を低くすると、ガチガチの正解ばかり選ぶお堅い回答になる。温度を高くすると、あえて2番目3番目に確率が高い言葉もランダムに選ぶようになって、多様でクリエイティブな文章を作れるようになるのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "なるほど！ わざと少し外した確率のものを選ぶから、AIは小説みたいなクリエイティブな文章も書けるのね。"
                }
            ]
        },
        {
            "id": 22,
            "title": "Hallucination",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "でも、ここに重大な落とし穴があるのだ。AIがもっともらしい嘘を堂々とついてしまう、ハルシネーション、幻覚という現象なのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "あ！ 歴史の出来事とかで大嘘を教えられたこと、私もあるわ！ あれはどうして起きるの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "AIはデータベースから事実を検索して答えているわけじゃないからなのだ。AIがやっているのは、あくまで文脈として次に来ると自然な言葉を確率で選んでいるだけなのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "あっ！ 確率で選んだ言葉の並びが自然なだけであって、事実かどうかは確認してないってこと！？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "その通り！ だからAIは、自分の知らないことでも、手持ちの言葉の確率を繋ぎ合わせて、いかにもありそうな自然な嘘の文を作ってしまうのだ。これはAIの根本的な仕組みによる、避けられない必然なのだ！"
                }
            ]
        },
        {
            "id": 23,
            "title": "なぜ賢く見える",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "知ってる知識を検索して答えるんじゃなくて、次にこの言葉が自然っぽいから出しちゃえってやってるのね。だからあんなに堂々と嘘をつけるんだ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "でもめたん。ただ確率で次の文字を当ててるだけのシステムが、どうしてプログラミングができたり、高度なクイズに答えられたりするのか不思議じゃないのだ？ この魔法を完成させるために、AIは想像を絶する学習というステップを踏んでいるのだ！"
                }
            ]
        },
        {
            "id": 24,
            "title": "Pre-training",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "確かに、いくら次はこれだって当てるシステムが完璧でも、そもそもAI自身に基礎知識がないとトンチンカンな言葉しか予測できないわよね。どうやってあんなに賢くなったの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "LLMの学習は、大きく3つの段階に分かれているのだ。これを基礎教育、専門教育、先輩の親身な指導に分けて説明するのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "おお、学校みたいね。わかりやすい！"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "まず第1段階は事前学習、プレトレーニング。これが基礎教育なのだ。インターネット上にある何兆というものすごい量のテキストデータを読み込ませて、ひたすら一部を隠して、次の文字を予測させるクイズをやらせるのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ネットの文章を片っ端から読み込ませるのね。"
                }
            ]
        },
        {
            "id": 25,
            "title": "Scale",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "最初はランダムに出力して間違えるけど、間違えたらAIの内部の数値をちょこっと修正するという作業を何兆回と繰り返すのだ。そうすると、AIは単なる暗記ではなく、言葉の背後にあるパターンや世界の知識までを、巨大な数式の重み、パラメータとして獲得するのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "丸暗記じゃなくて、法則を汎化していくってことね！"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "ちなみに、AIの賢さの規模はこのパラメータの数で表されるのだ。2018年の初代GPT-1は1億ちょっとだったけど、2020年のGPT-3はなんと1750億個！ そして今のGPT-4は兆を超えているとも言われているのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "規模の桁が違いすぎるわ！ 莫大すぎる！"
                }
            ]
        },
        {
            "id": 26,
            "title": "Fine-tuning",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "で、第2段階がファインチューニング、微調整。専門教育なのだ。ネットの文章を読んだだけのAIは、まだ使い勝手が悪いのだ。だから質問にはこうやって対話形式で答えるんだよという質の高いお手本データを読ませて、対話の仕方を微調整するのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "なるほど、オタク知識を蓄えただけの状態から、接客マニュアルを教え込むのね。"
                }
            ]
        },
        {
            "id": 27,
            "title": "RLHF",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "そして最後の第3段階、ここがChatGPTをあんなに人間っぽくしてブレイクさせた最大の鍵なのだ。RLHF、人間のフィードバックによる強化学習という、先輩からの熱血指導なのだ！"
                },
                {
                    "speaker": "めたん",
                    "text": "人間のフィードバック？ 人間が直接指導するの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "そうなのだ！ AIに出させた複数の答えを、人間のスタッフがこっちの答えの方が親切だね、こっちの答えは倫理的にアウト！ってひたすら採点するのだ。その人間の採点基準を学習した報酬モデルという採点専用AIを作って、メインのAIをさらに特訓させるのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ベースの部分には人間の生の手作業がしっかり入っているのね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "だからChatGPTは、ただ確率で言葉を繋ぐだけの冷たい機械から、僕たち人間が心地よい、役に立つと感じるように、徹底的に価値観を躾けられているのだ。"
                }
            ]
        },
        {
            "id": 28,
            "title": "3段階の学習",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "ただのネットの丸暗記じゃなくて、予測の法則を学んで、会話の仕方を学んで、人間の価値観まで教え込まれている。だからあんなに自然なんだわ！"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "インプットからアウトプット、そして学習の仕組みまで、これで全ての道のりを辿り終えたのだ！ ここまでのすべてを振り返ると、最初のあの問いの答えが、はっきりと見えてくるはずなのだ。"
                }
            ]
        },
        {
            "id": 29,
            "title": "全体フロー",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "最初の問いって、ChatGPTは文章の意味を1ミリも理解していない、ってやつよね。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "そうなのだ。今日見てきたプロセスを振り返るのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "ええと、入力された文字はトークン化されて埋め込みベクトルという数字の座標になり、Attentionでお互いの関係性を計算される。そして最後はその結果を取り込んで、次に来る1文字の確率を予測するのをループさせて文章をつなげていく。"
                }
            ]
        },
        {
            "id": 30,
            "title": "イリュージョン",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "ずんだもん",
                    "text": "まさにそうなのだ。AIの内部には心を込めた配慮も事実を格納した辞書もない。やっていることは、巨大な数字ベクトル同士の計算と、次はこの文字が来る確率が高いという予測処理だけなのだ。"
                },
                {
                    "speaker": "めたん",
                    "text": "本当に、徹底的な確率的オウムだったのね。でも、それなのにあんなに賢く見えるのはどうしてなの？"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "それは、学習させたデータ量とパラメータ数が想像を絶する超巨大スケールだからなのだ。インターネット上のありとあらゆる人類の言葉のパターンを吸収しすぎた結果、統計的に最も自然な言葉の連なりを出力すると、それが人間から見ればまるで完璧に意味を理解しているかのように見えてしまうという、ある種のイリュージョンなのだ！"
                }
            ]
        },
        {
            "id": 31,
            "title": "正しい距離感",
            "hold_sec": 0,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "凄い規模の話ね。本当に意味なんてわからなくても、パターンの抽出を極致までやれば、それは表面上は知能と見分けがつかないってことだわ。"
                },
                {
                    "speaker": "ずんだもん",
                    "text": "AIは理解しているわけではなく、確率で次の言葉を予測しているだけ。という本当の姿を理解して、正しい距離感でAIを使いたおしていくのだ！ それでは今日の解説はここまでなのだ！"
                }
            ]
        },
        {
            "id": 32,
            "title": "エンディング",
            "hold_sec": 3,
            "lines": [
                {
                    "speaker": "めたん",
                    "text": "次回の動画も絶対見てね！ バイバーイ！"
                }
            ]
        }
    ]
}